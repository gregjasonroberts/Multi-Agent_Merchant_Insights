# -*- coding: utf-8 -*-
"""agentic_ai_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlPso5NoPuGqdFjA8NoD0ZclQAA6zeJH
"""

# !pip install -r requirements.txt
!pip install pandas numpy fpdf pdfplumber python-dateutil matplotlib seaborn pymupdf

req_path = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/requirements.txt"
!pip install -r "$req_path"
with open(req_path) as f: print(f.read())
!pip install --upgrade openai

"""### Imports"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pickle
import fitz
from dotenv import load_dotenv
import os
import re

# Load env vars from .env file
load_dotenv()

# Now set OpenAI key
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

#testing env
import os
print(os.getcwd())

from dotenv import load_dotenv
load_dotenv("/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/.env")

import os
print("✅", os.getenv("OPENAI_API_KEY")[:8], "loaded...")

"""### Create randomized 6mo sample merchant daily stats


"""

def generate_merchant_daily_stats(
    merchant_ids,
    industries_map,
    start_date="2025-01-01",
    num_days=180,
    seed=42,
    behavior_map=None
):
    np.random.seed(seed)
    date_range = pd.date_range(start=start_date, periods=num_days, freq="D")
    data = []

    for merchant_id in merchant_ids:
        industry = industries_map.get(merchant_id, "Unknown")
        behavior = behavior_map.get(merchant_id, {}) if behavior_map else {}

        # Default values
        base_sales = behavior.get("base_sales", 5000)
        sales_decline_rate = behavior.get("sales_decline_rate", 0)  # e.g., 0.01 = 1% daily decline
        base_txns = behavior.get("base_txns", 80)
        txn_variability = behavior.get("txn_variability", 10)

        for i, date in enumerate(date_range):
            daily_sales = base_sales * ((1 - sales_decline_rate) ** i)
            daily_sales += np.random.normal(0, 300)  # small noise

            moi_transactions = np.random.randint(
                base_txns - txn_variability,
                base_txns + txn_variability
            )
            moi_ats = daily_sales / moi_transactions if moi_transactions > 0 else 0

            peer_sales = np.random.normal(loc=5200, scale=900)
            peer_transactions = np.random.randint(65, 95)
            peer_ats = peer_sales / peer_transactions if peer_transactions > 0 else 0

            data.append({
                "merchant_id": merchant_id,
                "industry": industry,
                "date": date.strftime("%Y-%m-%d"),
                "moi_ATS": round(moi_ats, 2),
                "moi_sales": round(daily_sales, 2),
                "moi_transactions": moi_transactions,
                "peer_ats": round(peer_ats, 2),
                "peer_sales": round(peer_sales, 2),
                "peer_transactions": peer_transactions
            })

    return pd.DataFrame(data)

merchant_ids = ["M001", "M002", "M003", "M004", "M005"]

industries_map = {
    "M001": "Restaurants",
    "M002": "Services",
    "M003": "Retail",
    "M004": "Restaurants",
    "M005": "Retail"
}

behavior_map = {
    "M001": {"base_sales": 6000, "sales_decline_rate": 0.01},  # 1% daily drop
    "M002": {"base_sales": 5000, "txn_variability": 5},        # stable
    "M003": {"base_sales": 5500, "txn_variability": 20},       # more volatile
    "M004": {"base_sales": 6500, "sales_decline_rate": 0.005}, # slower decline
    "M005": {"base_sales": 4800, "txn_variability": 10}        # moderate variance
}

stats_df = generate_merchant_daily_stats(
    merchant_ids,
    industries_map,
    behavior_map=behavior_map
)

stats_df.head()

"""# Building our orchestration agents

## Ingestion Agents
"""

import fitz  # PyMuPDF
import pandas as pd
import re

class PlanIngestionAgent:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path

    def load(self):
        doc = fitz.open(self.pdf_path)
        all_sector_dfs = []

        for page_num, page in enumerate(doc):
            print(f"\n📄 Sector: Attempting detection | Page: {page_num}")
            rows = self._extract_table_rows(page)

            if len(rows) < 3:
                print(f"⚠️ Skipping page {page_num}: not enough rows.")
                continue

            title_row = rows[0]
            headers = rows[1]
            data_rows = rows[2:]

            # Try to detect sector name from title
            sector = "Unknown"
            for cell in title_row:
                match = re.search(r"(Restaurants|Services|Retail)", cell, re.IGNORECASE)
                if match:
                    sector = match.group(1).capitalize()
                    break

            print(f"✅ Detected Sector: {sector}")
            print(f"📝 Headers: {headers}")
            print(f"📊 Sample Row: {data_rows[0] if data_rows else 'None'}")

            try:
                df = pd.DataFrame(data_rows, columns=headers)
                df["sector"] = sector
                all_sector_dfs.append(df)
            except Exception as e:
                print(f"❌ Failed to create DataFrame on page {page_num}: {e}")
                continue

        if not all_sector_dfs:
            raise ValueError("❌ No valid tables parsed from plan matrix PDF.")

        combined_df = pd.concat(all_sector_dfs, ignore_index=True)

        # ✅ Normalize and rename feature column
        combined_df.columns = [col.lower() for col in combined_df.columns]
        if combined_df.columns[0] != "feature":
            combined_df.rename(columns={combined_df.columns[0]: "feature"}, inplace=True)

        return combined_df


    def _extract_table_rows(self, page):
        # Use structured layout
        words = page.get_text("dict")["blocks"]
        rows_dict = {}

        for block in words:
            if "lines" not in block:
                continue
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text:
                        continue
                    y = round(span["bbox"][1])
                    x = span["bbox"][0]
                    if y not in rows_dict:
                        rows_dict[y] = []
                    rows_dict[y].append((x, text))

        # Sort top-to-bottom, left-to-right
        rows = []
        for y in sorted(rows_dict.keys()):
            row = sorted(rows_dict[y], key=lambda x: x[0])
            row_text = [text for _, text in row]
            rows.append(row_text)

        return rows

class MetadataIngestionAgent:
    def __init__(self, csv_path):
        self.csv_path = csv_path

    def load_and_clean(self):
        df = pd.read_csv(self.csv_path)

        # Check required columns
        required_columns = ['merchant_id', 'industry', 'location', 'current_plan', 'active_services']
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            raise ValueError(f"Missing columns in metadata: {missing}")

        df['merchant_id'] = df['merchant_id'].astype(str)

        # Split active_services if comma-separated
        df['active_services'] = df['active_services'].apply(
            lambda x: [service.strip() for service in str(x).split(',')] if pd.notna(x) else []
        )

        return df

# segmentation ingestion

class AllMerchantsSegmentationAgent:
    def __init__(self, csv_path: str):
        self.csv_path = csv_path

    def load_and_clean(self) -> pd.DataFrame:
        """
        Loads the all_merchants_segments CSV and ensures the following columns exist:
          - merchant_id
          - name
          - industry
          - location
          - is_customer
          - peer_group_id

        Casts merchant_id and peer_group_id to str, is_customer to bool.
        """
        df = pd.read_csv(self.csv_path)

        # Required columns
        required = [
            "merchant_id",
            "name",
            "industry",
            "location",
            "is_customer",
            "peer_group_id"
        ]
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"Missing columns in all_merchants_segments: {missing}")

        # Normalize types
        df["merchant_id"]   = df["merchant_id"].astype(str)
        df["peer_group_id"] = df["peer_group_id"].astype(str)
        # Convert common truthy/falsy values to boolean
        df["is_customer"]   = df["is_customer"].astype(bool)

        return df

class DailyStatsIngestionAgent:
    def __init__(self, csv_path=None, stats_df=None):
        self.csv_path = csv_path
        self.stats_df = stats_df

    def load_and_process(self):
        if self.stats_df is not None:
            print("📊 Using in-memory `stats_df` DataFrame")
            df = self.stats_df.copy()
        elif self.csv_path is not None:
            print(f"📄 Loading daily stats from {self.csv_path}")
            df = pd.read_csv(self.csv_path)
        else:
            raise ValueError("Either stats_df or csv_path must be provided.")

        # Ensure date is parsed
        df['date'] = pd.to_datetime(df['date'])

        # Confirm required columns
        expected = ['merchant_id', 'industry', 'date',
                    'moi_ATS', 'moi_sales', 'moi_transactions',
                    'peer_ats', 'peer_sales', 'peer_transactions']
        missing = [col for col in expected if col not in df.columns]
        if missing:
            raise ValueError(f"Missing columns in daily stats: {missing}")

        return df

"""### Data ingestion control agent"""

class DataIngestionManagerAgent:
    def __init__(
        self,
        plan_agent,
        metadata_agent,
        stats_agent,
        segmentation_agent
    ):
        self.plan_agent = plan_agent
        self.metadata_agent = metadata_agent
        self.stats_agent = stats_agent
        self.segmentation_agent = segmentation_agent

    def run_ingestion(self):
        print("📥 Ingesting merchant metadata...")
        merchant_df = self.metadata_agent.load_and_clean()

        print("📥 Ingesting daily stats...")
        # Supports either CSV-path or in-memory DataFrame
        if hasattr(self.stats_agent, "load_and_process"):
            stats_df = self.stats_agent.load_and_process()
        else:
            stats_df = self.stats_agent.load()

        print("📥 Ingesting service plan matrix...")
        plan_matrix = self.plan_agent.load()

        print("📥 Ingesting all-merchants segmentation...")
        all_merchants_df = self.segmentation_agent.load_and_clean()

        print("✅ All data ingested.")
        return {
            "merchant_df": merchant_df,
            "stats_df": stats_df,
            "plan_matrix": plan_matrix,
            "all_merchants_df": all_merchants_df
        }

"""### Running test merchants"""

#Import my data sources
PLAN_PDF_PATH = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/service_plan_matrix_structured_final.pdf"
METADATA_CSV_PATH = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/merchant_metadata.csv"
SEGMENT_CSV_PATH = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/all_merchants_segments.csv"

# Agents
plan_agent = PlanIngestionAgent(PLAN_PDF_PATH)
metadata_agent = MetadataIngestionAgent(METADATA_CSV_PATH)
merchant_df = metadata_agent.load_and_clean()
segmentation_agent = AllMerchantsSegmentationAgent(SEGMENT_CSV_PATH)

stats_agent = DailyStatsIngestionAgent(stats_df=stats_df)


# Manager
manager = DataIngestionManagerAgent(plan_agent, metadata_agent, stats_agent, segmentation_agent)

# Run
data_bundle = manager.run_ingestion()

print("✅ Modules loaded successfully.")

#save for later

with open("/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/data/data_bundle.pkl", "wb") as f:
    pickle.dump(data_bundle, f)

#read back in

with open("/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/data/data_bundle.pkl", "rb") as f:
    data_bundle = pickle.load(f)

"""### Peers agent"""

class PeerStatsAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_relative_performance(self, metric="moi_sales", periods=[30, 90, 180]):
        relative_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_metrics = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                moi_mean = window[metric].mean()
                peer_metric = "peer_" + metric.split("_")[1]
                peer_mean = window[peer_metric].mean()

                delta = moi_mean - peer_mean
                rel_perf = "Above Peer" if delta > 0 else "Below Peer" if delta < 0 else "Equal"

                merchant_metrics[f"{days}d"] = {
                    "moi_avg": round(moi_mean, 2),
                    "peer_avg": round(peer_mean, 2),
                    "difference": round(delta, 2),
                    "relative_performance": rel_perf
                }

            relative_results[merchant_id] = merchant_metrics

        return relative_results

data_bundle.keys()

peer_agent = PeerStatsAgent(data_bundle["stats_df"])
peer_trends = peer_agent.compute_relative_performance(metric="moi_sales")

"""### Sales trend agent"""

import numpy as np

class SalesTrendAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_sales_trend(self, periods=[30, 90, 180]):
        trend_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_trends = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                x = np.arange(len(window))
                y = window["moi_sales"].values

                slope, _ = np.polyfit(x, y, deg=1)

                if slope > 5:
                    trend = "Increasing"
                elif slope < -5:
                    trend = "Decreasing"
                else:
                    trend = "Flat"

                merchant_trends[f"{days}d"] = {
                    "slope": round(slope, 2),
                    "trend": trend
                }

            trend_results[merchant_id] = merchant_trends

        return trend_results

sales_agent = SalesTrendAgent(data_bundle["stats_df"])
sales_trends = sales_agent.compute_sales_trend()

sales_trends

"""### Transaction trend agent"""

class TransactionTrendAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_transaction_trend(self, periods=[30, 90, 180]):
        trend_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_trends = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                x = np.arange(len(window))
                y = window["moi_transactions"].values

                slope, _ = np.polyfit(x, y, deg=1)

                if slope > 2:
                    trend = "Increasing"
                elif slope < -2:
                    trend = "Decreasing"
                else:
                    trend = "Flat"

                merchant_trends[f"{days}d"] = {
                    "slope": round(slope, 2),
                    "trend": trend
                }

            trend_results[merchant_id] = merchant_trends

        return trend_results

transaction_agent = TransactionTrendAgent(data_bundle["stats_df"])
transaction_trends = transaction_agent.compute_transaction_trend()

"""### ATS trend agent"""

import numpy as np

class ATSTrendAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_ats_trend(self, periods=[30, 90, 180]):
        trend_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_trends = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                x = np.arange(len(window))
                y = window["moi_ATS"].values

                slope, _ = np.polyfit(x, y, deg=1)

                if slope > 0.5:
                    trend = "Increasing"
                elif slope < -0.5:
                    trend = "Decreasing"
                else:
                    trend = "Flat"

                merchant_trends[f"{days}d"] = {
                    "slope": round(slope, 2),
                    "trend": trend
                }

            trend_results[merchant_id] = merchant_trends

        return trend_results

ats_agent = ATSTrendAgent(data_bundle["stats_df"])
ats_trends = ats_agent.compute_ats_trend()

class WeekdayProfileAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()
        self.df["weekday"] = self.df["date"].dt.day_name()

    def analyze(self):
        profiles = {}

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id]
            weekday_group = merchant_df.groupby("weekday")["moi_sales"].mean()
            peer_group = merchant_df.groupby("weekday")["peer_sales"].mean()

            closed_days = weekday_group[weekday_group == 0].index.tolist()
            strong_days = weekday_group[weekday_group > weekday_group.mean()].index.tolist()
            low_days = weekday_group[weekday_group < weekday_group.mean()].index.tolist()

            gap_vs_peer = (weekday_group - peer_group).sort_values()
            biggest_gap_day = gap_vs_peer.idxmin()  # most negative difference

            profiles[merchant_id] = {
                "closed_days": closed_days,
                "low_performance_days": low_days,
                "strong_days": strong_days,
                "day_with_biggest_gap_vs_peer": biggest_gap_day,
                "avg_sales_by_day": weekday_group.to_dict()
            }

        return profiles

weekday_agent = WeekdayProfileAgent(data_bundle["stats_df"])
weekday_profiles = weekday_agent.analyze()

# View one merchant
import pprint
pprint.pprint(weekday_profiles["M005"])

"""## Insights Agents"""

class InsightManagerAgent:
    def __init__(self, sales_trends, transaction_trends, ats_trends, peer_stats=None):
        self.sales_trends = sales_trends
        self.transaction_trends = transaction_trends
        self.ats_trends = ats_trends
        self.peer_stats = peer_stats if peer_stats else {}

    def generate_combined_insights(self):
        merchants = self.sales_trends.keys()
        insights = {}

        for merchant_id in merchants:
            merchant_insight = {}

            for period in ["30d", "90d", "180d"]:
                sale_trend = self.sales_trends.get(merchant_id, {}).get(period, {}).get("trend")
                txn_trend = self.transaction_trends.get(merchant_id, {}).get(period, {}).get("trend")
                ats_trend = self.ats_trends.get(merchant_id, {}).get(period, {}).get("trend")

                summary = f"Period: {period} — "
                components = []

                if sale_trend:
                    components.append(f"Sales are {sale_trend.lower()}")
                if txn_trend:
                    components.append(f"foot traffic is {txn_trend.lower()}")
                if ats_trend:
                    components.append(f"average ticket size is {ats_trend.lower()}")

                summary += ", ".join(components)

                # Optional: derive a simplified explanation
                if sale_trend == "Decreasing" and txn_trend == "Flat" and ats_trend == "Decreasing":
                    summary += " — likely due to pricing or discounts dropping."
                elif sale_trend == "Decreasing" and txn_trend == "Decreasing" and ats_trend == "Flat":
                    summary += " — decline likely driven by lower foot traffic."

                merchant_insight[period] = summary

            insights[merchant_id] = merchant_insight

        return insights

"""### Run Insights"""

insight_mgr = InsightManagerAgent(
    sales_trends=sales_trends,
    transaction_trends=transaction_trends,  # fixed name
    ats_trends=ats_trends,
    peer_stats=peer_trends  # optional
)


combined_insights = insight_mgr.generate_combined_insights()

# import openai

# class LLM_InsightExplainerAgent:
#     def __init__(self, model="gpt-4", temperature=0.3):
#         self.model = model
#         self.temperature = temperature

#     def generate_prompt(self, merchant_id, sales, txns, ats, weekday_profile):
#         return f"""
#             You are a business insight assistant. Use the following trend summaries and weekday behavior for merchant {merchant_id} to write a short insight (2–4 sentences).

#             Sales Trends: {sales}
#             Transaction Trends: {txns}
#             Average Ticket Size Trends: {ats}

#             Weekday Profile:
#             - Closed Days: {weekday_profile.get("closed_days", [])}
#             - Strong Days: {weekday_profile.get("strong_days", [])}
#             - Low Performance Days: {weekday_profile.get("low_performance_days", [])}
#             - Biggest Gap vs. Peers: {weekday_profile.get("day_with_biggest_gap_vs_peer")}

#             Write a helpful, business-aware insight that reflects meaningful trends. If the merchant is closed on certain days, avoid penalizing them for it.
#             """

#     def explain(self, merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile):
#         prompt = self.generate_prompt(
#             merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile
#         )

#         response = openai.ChatCompletion.create(
#             model=self.model,
#             temperature=self.temperature,
#             messages=[
#                 {"role": "system", "content": "You are a business performance analyst."},
#                 {"role": "user", "content": prompt}
#             ]
#         )

#         return response.choices[0].message["content"].strip()
###

from openai import OpenAI
import os

class LLM_InsightExplainerAgent:
    def __init__(self, model="gpt-4", temperature=0.3):
        self.model = model
        self.temperature = temperature
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_prompt(self, merchant_id, sales, txns, ats, weekday_profile):
        return f"""
          You are a business insight assistant. Use the following trend summaries and weekday behavior for merchant {merchant_id} to write a short insight (2–4 sentences).

          Sales Trends: {sales}
          Transaction Trends: {txns}
          Average Ticket Size Trends: {ats}

          Weekday Profile:
          - Closed Days: {weekday_profile.get("closed_days", [])}
          - Strong Days: {weekday_profile.get("strong_days", [])}
          - Low Performance Days: {weekday_profile.get("low_performance_days", [])}
          - Biggest Gap vs. Peers: {weekday_profile.get("day_with_biggest_gap_vs_peer")}

          Write a helpful, business-aware insight that reflects meaningful trends. If the merchant is closed on certain days, avoid penalizing them for it.
          """

    def explain(self, merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile):
        prompt = self.generate_prompt(
            merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile
        )

        response = self.client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            messages=[
                {"role": "system", "content": "You are a business performance analyst."},
                {"role": "user", "content": prompt}
            ]
        )

        return response.choices[0].message.content.strip()

"""### Process insights"""

llm_explainer = LLM_InsightExplainerAgent()

llm_insights = {}

for merchant_id in sales_trends:
    try:
        llm_insights[merchant_id] = llm_explainer.explain(
            merchant_id,
            sales_trends[merchant_id],
            transaction_trends[merchant_id],
            ats_trends[merchant_id],
            weekday_profiles[merchant_id]
        )
        print(f"✅ {merchant_id} processed")
    except Exception as e:
        print(f"⚠️ Error for {merchant_id}: {e}")

for m_id, insight in llm_insights.items():
    print(f"\n🔍 {m_id}:\n{insight}")

from openai import OpenAI
import os

class LLM_ActionabilityAgent:
    def __init__(self, model="gpt-4", temperature=0):
        self.model = model
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.temperature = temperature

    def generate_prompt(self, merchant_id, insight):
        return f"""
        You are a business strategist reviewing the following insight about merchant {merchant_id}:

        Insight:
        \"\"\"{insight}\"\"\"

        Classify the insight as one of the following:
        - Actionable: Clear opportunity to improve performance with a recommendation.
        - Not Actionable: Performance is fine or the insight reflects stable business patterns.
        - Neutral: There's an observation, but no recommendation is needed.

        Respond with only the classification term: Actionable, Not Actionable, or Neutral.
        """

    def classify(self, merchant_id, insight):
        prompt = self.generate_prompt(merchant_id, insight)

        response = self.client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            messages=[
                {"role": "system", "content": "You are a helpful business strategist."},
                {"role": "user", "content": prompt}
            ]
        )

        return response.choices[0].message.content.strip()

actionability_agent = LLM_ActionabilityAgent()
actionability_labels = {}

for m_id, insight in llm_insights.items():
    try:
        label = actionability_agent.classify(m_id, insight)
        actionability_labels[m_id] = label
        print(f"🟩 {m_id}: {label}")
    except Exception as e:
        print(f"⚠️ Error for {m_id}: {e}")

from pprint import pprint

combined = {
    m_id: {
        "insight": llm_insights[m_id],
        "actionability": actionability_labels[m_id]
    }
    for m_id in llm_insights
}

pprint(combined)

"""# Plan manager"""

from openai import OpenAI
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

class LLMPlanMatcherAgent:
    def __init__(
        self,
        plan_matrix: pd.DataFrame,
        merchant_df: pd.DataFrame,
        all_merchants_df: pd.DataFrame,
        openai_api_key: str,
        similarity_threshold: float = 0.65
    ):
        """
        plan_matrix: DataFrame of plan features (features x plans x sector)
        merchant_df:  DataFrame of current customers (merchant_id, industry, location, current_plan, active_services)
        all_merchants_df: DataFrame of all merchants with `is_customer` flag
        """
        self.plan_matrix = plan_matrix
        self.merchant_df = merchant_df.set_index("merchant_id")
        self.all_merchants_df = all_merchants_df.set_index("merchant_id")
        self.similarity_threshold = similarity_threshold

        # OpenAI client
        self.client = OpenAI(api_key=openai_api_key)
        self.embedding_model = "text-embedding-ada-002"
        self.chat_model = "gpt-4"
        self.plans = ['basic', 'plus', 'pro', 'premium', 'advanced', 'ultimate']
        self.feature_column = 'feature'

        # Precompute feature embeddings
        self.feature_embeddings = self._precompute_feature_embeddings()

        def recommend_plan(self, merchant_id: str, insights: dict, peer_insights: dict = None) -> dict:
          # 0) If merchant is not in all_merchants_df or not a customer
          if (
              merchant_id not in self.all_merchants_df.index
              or not bool(self.all_merchants_df.loc[merchant_id, "is_customer"])
          ):
              # Skip proprietary plan matching
              return self._generate_generic_recommendation(merchant_id, insights, peer_insights)

          try:
              # 1) Semantic embedding match → plan
              sem_plan = self._semantic_match_plan(insights, merchant_id)
              if sem_plan:
                  narrative = self._generate_plan_recommendation_via_llm(
                      merchant_id, sem_plan, insights, peer_insights
                  )
                  return {
                      "merchant_id": merchant_id,
                      "recommended_plan": sem_plan.capitalize(),
                      "reason": narrative,
                      "fallback": False
                  }

              # 2) LLM plan classifier
              llm_plan = self._llm_plan_classifier(insights)
              if llm_plan:
                  narrative = self._generate_plan_recommendation_via_llm(
                      merchant_id, llm_plan, insights, peer_insights
                  )
                  return {
                      "merchant_id": merchant_id,
                      "recommended_plan": llm_plan,
                      "reason": narrative,
                      "fallback": False
                  }

              # 3) Generic fallback
              return self._generate_generic_recommendation(merchant_id, insights, peer_insights)

          except Exception as e:
              return {
                  "merchant_id": merchant_id,
                  "recommended_plan": "Generic Business Tip",
                  "reason": f"⚠️ Recommendation failed: {e}",
                  "fallback": True
              }


    def _precompute_feature_embeddings(self) -> dict:
        features = self.plan_matrix[self.feature_column].dropna().unique().tolist()
        resp = self.client.embeddings.create(
            model=self.embedding_model,
            input=features
        )
        return {feat: item.embedding for feat, item in zip(features, resp.data)}

    def _semantic_match_plan(self, insights, merchant_id) -> str:
        text = " ".join(insights.values()) if isinstance(insights, dict) else str(insights)
        resp = self.client.embeddings.create(model=self.embedding_model, input=[text])
        insight_emb = np.array(resp.data[0].embedding).reshape(1, -1)

        sims = [(feat, cosine_similarity(insight_emb, np.array(emb).reshape(1, -1))[0][0])
                for feat, emb in self.feature_embeddings.items()]
        top3 = sorted(sims, key=lambda x: x[1], reverse=True)[:3]
        print(f"🔍 Merchant {merchant_id} top-3 feature sims:", top3)

        matched = [feat for feat, score in sims if score >= self.similarity_threshold]
        if not matched:
            return None

        scores = {p: 0 for p in self.plans}
        df = self.plan_matrix
        for feat in matched:
            row = df[df[self.feature_column] == feat]
            if row.empty:
                continue
            for p in self.plans:
                if str(row.iloc[0][p]).upper() == 'X':
                    scores[p] += 1

        best, cnt = max(scores.items(), key=lambda x: x[1])
        return best if cnt > 0 else None

    def _llm_plan_classifier(self, insights) -> str:
        disp = self.plan_matrix[[self.feature_column] + self.plans].rename(
            columns={c: c.capitalize() for c in self.plans + [self.feature_column]}
        )
        matrix_str = disp.to_string(index=False)

        text = " ".join(insights.values()) if isinstance(insights, dict) else str(insights)
        prompt = f"""
Plans and features:
{matrix_str}

Merchant insight: "{text}"

Which one plan (Basic, Plus, Pro, Premium, Advanced, Ultimate) best addresses these needs?
If none applies, reply "None".
"""
        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role": "system", "content": "You are an expert plan recommender."},
                {"role": "user",   "content": prompt}
            ],
            temperature=0,
            max_tokens=10
        )
        plan = resp.choices[0].message.content.strip().capitalize()
        return plan if plan.lower() in self.plans else None

    def _generate_generic_recommendation(self, merchant_id: str, insights, peer_insights=None) -> dict:
        # Generic fallback for both non-customers and unmatched customers
        if merchant_id in self.merchant_df.index:
            m = self.merchant_df.loc[merchant_id]
            industry, location = m["industry"], m["location"]
        else:
            industry, location = "their industry", "their area"

        text = " ".join(insights.values()) if isinstance(insights, dict) else str(insights)
        prompt = (
            f"You are a small-business strategist for the {industry} sector in {location}.\n"
            f"Performance insights: {text}\n"
        )
        if peer_insights:
            prompt += f"Peer group comparison: {peer_insights.get(merchant_id, '')}\n"
        prompt += (
            "Provide one actionable recommendation that does NOT involve subscribing to a plan, "
            "tailored to improve sales, foot traffic, or ticket size."
        )

        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role":"system", "content":"You are a helpful business consultant."},
                {"role":"user",   "content":prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )
        advice = resp.choices[0].message.content.strip()

        return {
            "merchant_id": merchant_id,
            "recommended_plan": "Generic Business Tip",
            "reason": advice,
            "fallback": True
        }

"""### Plan matching"""

from openai import OpenAI
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

class LLMPlanMatcherAgent:
    def __init__(
        self,
        plan_matrix: pd.DataFrame,
        merchant_df: pd.DataFrame,
        all_merchants_df: pd.DataFrame,
        openai_api_key: str,
        similarity_threshold: float = 0.65
    ):
        """
        plan_matrix:       DataFrame of plan features (columns: feature, basic, plus, …, ultimate, sector)
        merchant_df:       DataFrame of subscribing customers (merchant_id, industry, location, current_plan, active_services)
        all_merchants_df:  DataFrame of all merchants with is_customer flag
        """
        self.plan_matrix = plan_matrix
        self.merchant_df = merchant_df.set_index("merchant_id")
        self.all_merchants_df = all_merchants_df.set_index("merchant_id")
        self.similarity_threshold = similarity_threshold

        # OpenAI client
        self.client = OpenAI(api_key=openai_api_key)
        self.embedding_model = "text-embedding-ada-002"
        self.chat_model = "gpt-4"

        # Plan tiers (lowercase)
        self.plans = ['basic', 'plus', 'pro', 'premium', 'advanced', 'ultimate']
        self.feature_column = 'feature'  # normalized lowercase

        # Precompute embeddings for each feature label
        self.feature_embeddings = self._precompute_feature_embeddings()

    def recommend_plan(self, merchant_id: str, insights: dict, peer_insights: dict = None) -> dict:
        # 0) If merchant is not in all_merchants_df or not a customer
        if (
            merchant_id not in self.all_merchants_df.index
            or not bool(self.all_merchants_df.loc[merchant_id, "is_customer"])
        ):
            # Skip proprietary plan matching
            return self._generate_generic_recommendation(merchant_id, insights, peer_insights)

        try:
            # 1) Semantic embedding match → plan
            sem_plan = self._semantic_match_plan(insights, merchant_id)
            if sem_plan:
                narrative = self._generate_plan_recommendation_via_llm(
                    merchant_id, sem_plan, insights, peer_insights
                )
                return {
                    "merchant_id": merchant_id,
                    "recommended_plan": sem_plan.capitalize(),
                    "reason": narrative,
                    "fallback": False
                }

            # 2) LLM plan classifier
            llm_plan = self._llm_plan_classifier(insights)
            if llm_plan:
                narrative = self._generate_plan_recommendation_via_llm(
                    merchant_id, llm_plan, insights, peer_insights
                )
                return {
                    "merchant_id": merchant_id,
                    "recommended_plan": llm_plan,
                    "reason": narrative,
                    "fallback": False
                }

            # 3) Generic fallback
            return self._generate_generic_recommendation(merchant_id, insights, peer_insights)

        except Exception as e:
            return {
                "merchant_id": merchant_id,
                "recommended_plan": "Generic Business Tip",
                "reason": f"⚠️ Recommendation failed: {e}",
                "fallback": True
            }


    def _precompute_feature_embeddings(self) -> dict:
        features = self.plan_matrix[self.feature_column].dropna().unique().tolist()
        resp = self.client.embeddings.create(
            model=self.embedding_model,
            input=features
        )
        return {feat: item.embedding for feat, item in zip(features, resp.data)}

    def _semantic_match_plan(self, insights: dict, merchant_id: str) -> str:
        text = " ".join(insights.values())
        resp = self.client.embeddings.create(
            model=self.embedding_model,
            input=[text]
        )
        insight_emb = np.array(resp.data[0].embedding).reshape(1, -1)

        sims = [
            (feat, cosine_similarity(insight_emb, np.array(emb).reshape(1, -1))[0][0])
            for feat, emb in self.feature_embeddings.items()
        ]
        # debug: top-3 sims
        top3 = sorted(sims, key=lambda x: x[1], reverse=True)[:3]
        print(f"🔍 Merchant {merchant_id} top-3 feature sims:", top3)

        matched = [feat for feat, score in sims if score >= self.similarity_threshold]
        if not matched:
            return None

        # score plans
        scores = {p: 0 for p in self.plans}
        df = self.plan_matrix
        for feat in matched:
            row = df[df[self.feature_column] == feat]
            if not row.empty:
                for p in self.plans:
                    if str(row.iloc[0][p]).upper() == 'X':
                        scores[p] += 1

        best, cnt = max(scores.items(), key=lambda x: x[1])
        return best if cnt > 0 else None

    def _llm_plan_classifier(self, insights: dict) -> str:
        # Build a small table for the prompt
        disp = self.plan_matrix[[self.feature_column] + self.plans].rename(
            columns={c: c.capitalize() for c in [self.feature_column] + self.plans}
        )
        matrix_str = disp.to_string(index=False)

        text = " ".join(insights.values())
        prompt = f"""
Service plans and features:
{matrix_str}

Merchant insight: "{text}"

Which one plan (Basic, Plus, Pro, Premium, Advanced, Ultimate) best addresses these needs?
If none applies, reply "None".
"""
        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role":"system", "content":"You are an expert plan recommender."},
                {"role":"user",   "content":prompt}
            ],
            temperature=0,
            max_tokens=10
        )
        plan = resp.choices[0].message.content.strip().capitalize()
        return plan if plan.lower() in self.plans else None

    def _generate_plan_recommendation_via_llm(
        self,
        merchant_id: str,
        plan: str,
        insights: dict,
        peer_insights: dict = None
    ) -> str:
        """
        Agentic step: craft a customer-facing paragraph explaining why
        upgrading to `plan` is the right move.
        """
        m = self.merchant_df.loc[merchant_id]
        industry, location = m["industry"], m["location"]
        insight_text = " ".join(insights.values())
        peer_text = f"\nPeer comparison: {peer_insights[merchant_id]}" if peer_insights else ""

        prompt = f"""
You are a small-business growth consultant for a {industry} in {location}.
Their insights: {insight_text}{peer_text}

We recommend upgrading to the {plan.capitalize()} plan.
Write one concise paragraph that:
- Explains why this plan fits their needs.
- Calls out 1–2 key features by name and how they help.
- Is friendly and actionable for the customer.

Respond with only the paragraph.
"""
        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role":"system", "content":"You are a helpful business strategist."},
                {"role":"user",   "content":prompt.strip()}
            ],
            temperature=0.7,
            max_tokens=200
        )
        return resp.choices[0].message.content.strip()

    def _generate_generic_recommendation(
        self,
        merchant_id: str,
        insights: dict,
        peer_insights: dict = None
    ) -> dict:
        # same generic fallback as before
        m = self.merchant_df.loc[merchant_id] if merchant_id in self.merchant_df.index else {}
        industry, location = m.get("industry", "their industry"), m.get("location", "their area")
        text = " ".join(insights.values())
        prompt = (
            f"You are a consultant for the {industry} sector in {location}.\n"
            f"Insights: {text}\n"
        )
        if peer_insights:
            prompt += f"Peer comparison: {peer_insights[merchant_id]}\n"
        prompt += (
            "Provide one actionable recommendation that does NOT involve subscribing to a plan."
        )

        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role":"system","content":"You are a helpful business consultant."},
                {"role":"user",  "content":prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )
        advice = resp.choices[0].message.content.strip()

        return {
            "merchant_id": merchant_id,
            "recommended_plan": "Generic Business Tip",
            "reason": advice,
            "fallback": True
        }

"""# New Section"""

for sector, table in data_bundle["plan_matrix"].items():
    print(f"\n🔍 Sector: {sector}")
    print(f"Type: {type(table)}")
    print("Preview keys:", list(table.keys())[:5])
    if isinstance(table, dict):
        first_key = next(iter(table))
        print(f"Sample row at '{first_key}':", table[first_key])

"""### Instantiate plans"""

plan_matrix = plan_agent.load()
# Normalize column names (lowercase and strip spaces)
plan_matrix.columns = [col.lower().strip() for col in plan_matrix.columns]

# Rename first column to 'feature' if needed
first_col = plan_matrix.columns[0]
if first_col != "feature":
    print(f"🔁 Renaming column '{first_col}' → 'feature'")
    plan_matrix.rename(columns={first_col: "feature"}, inplace=True)

# print("🧩 Columns in plan_matrix:", plan_matrix.columns.tolist())
print("🧩 plan_matrix columns:", plan_matrix.columns.tolist())
print("📌 Sample row 0:")
print(plan_matrix.head(1))

"""### Run pipelne reecommendations

```
`# This is formatted as code`
```


"""

#Add all_merchants and run pipleine

# Paths


# Instantiate and load
# seg_agent = AllMerchantsSegmentationAgent(SEGMENT_CSV)
# all_merchants_df = seg_agent.load_and_clean()
insight_mgr = InsightManagerAgent(
    sales_trends=sales_trends,
    transaction_trends=transaction_trends,
    ats_trends=ats_trends,
    peer_stats=peer_trends
)

insights = insight_mgr.generate_combined_insights()

matcher = LLMPlanMatcherAgent(
    plan_matrix      = data_bundle["plan_matrix"],
    merchant_df      = data_bundle["merchant_df"],
    all_merchants_df = data_bundle["all_merchants_df"],
    openai_api_key   = os.getenv("OPENAI_API_KEY"),
)

# 6) Generate recommendations
results = [
    matcher.recommend_plan(mid, insights[mid], peer_insights=peer_trends)
    for mid in insights
]
import pandas as pd
pd.DataFrame(results)


# # Generate recommendations
recommendations = []
for merchant_id in insights.keys():
    try:
        rec = matcher.recommend_plan(
            merchant_id=merchant_id,
            insights=insights[merchant_id],
            peer_insights=peer_trends
        )
        recommendations.append(rec)
    except Exception as e:
        print(f"⚠️ Failed for {merchant_id}: {e}")

# Display all recommendations
import pandas as pd
recommendation_df = pd.DataFrame(recommendations)
print(recommendation_df)

insights

import pandas as pd


# 1) Build a DataFrame of the recs
rec_df = pd.DataFrame(recommendations)

# 2) Rename 'reason' → 'recommendation'
rec_df = rec_df.rename(columns={'reason': 'recommendation'})

# 3) Create an 'insight' column by flattening the dict into one string
rec_df['insight'] = rec_df['merchant_id'].map(
    lambda mid: " | ".join(f"{period}: {txt}" for period,txt in insights[mid].items())
)

# 4) Reorder columns
rec_df = rec_df[[
    'merchant_id',
    'insight',
    'recommended_plan',
    'recommendation',
    'fallback'
]]

# 5) (Optional) Show full text
pd.set_option('display.max_colwidth', None)

rec_df

"""# Finalize Recommendation"""

from openai import OpenAI

class RecommendationCritiqueAgent:
    def __init__(self, openai_api_key: str, model: str = "gpt-4"):
        """
        Agent to critique and compress a verbose recommendation
        into a single, action-oriented sentence.
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.chat_model = model

    def critique(self, merchant_id: str, verbose_text: str) -> str:
        prompt = f"""
You are a professional small business expert.
Here is a customer-facing recommendation for merchant {merchant_id}:

\"\"\"{verbose_text}\"\"\"

Please rewrite this as **one concise sentence** that:
- Keeps the same key advice and plan name,
- Is a non pushy marketing output is straight to the point and actionable,
- Removes all extra explanation,
- Do not use merchant ID in the output.

Respond with only that single sentence.
"""
        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role": "system", "content": "You are a helpful, precise writing assistant."},
                {"role": "user",   "content": prompt.strip()}
            ],
            temperature=0.3,
            max_tokens=60
        )
        return resp.choices[0].message.content.strip()

# 1) Instantiate critique agent
critique_agent = RecommendationCritiqueAgent(os.getenv("OPENAI_API_KEY"))

# 2) Apply to each recommendation
for rec in recommendations:
    # Only critique proprietary-plan recs, or all if you like
    if not rec["fallback"]:
        rec["concise_recommendation"] = critique_agent.critique(
            rec["merchant_id"],
            rec["reason"]  # your verbose recommendation text
        )
    else:
        # Optionally leave the generic tip as-is or also compress it
        rec["concise_recommendation"] = critique_agent.critique(
            rec["merchant_id"],
            rec["reason"]
        )

# 3) Build final DataFrame
import pandas as pd
df = pd.DataFrame(recommendations)


# Build the insight Series
insight_series = df['merchant_id'].map(
    lambda mid: " | ".join(f"{period}: {txt}" for period, txt in insights[mid].items())
)

# 1) Remove any existing 'insight' (if you ran this before)
if 'insight' in df.columns:
    df.drop(columns=['insight'], inplace=True)

# 2) Insert it at position 1 (i.e. just after merchant_id)
df.insert(loc=1, column='insight', value=insight_series)

# 3) (Re-)order the rest if you like
df = df[[
    "merchant_id",
    "insight",
    "fallback",
    "recommended_plan",
    "reason",
    "concise_recommendation"
]]

pd.set_option('display.max_colwidth', None)
print(df)

df.head()

import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

# Create an even larger figure canvas
fig, ax = plt.subplots(figsize=(24, 16))
ax.axis('off')

# Define agents with underscores and precise, aligned positions
agents = {
    "Plan_Ingestion_Agent": (0.125, 0.9),
    "Metadata_Ingestion_Agent": (0.375, 0.9),
    "Daily_Stats_Ingestion_Agent": (0.625, 0.9),
    "All_Merchants_Segmentation_Agent": (0.875, 0.9),
    "Data_Ingestion_Manager_Agent": (0.5, 0.75),
    "Orchestrator_Agent": (0.5, 0.6),
    "Sales_Trend_Agent": (0.1, 0.45),
    "Transaction_Trend_Agent": (0.3, 0.45),
    "ATS_Trend_Agent": (0.5, 0.45),
    "Peer_Stats_Agent": (0.7, 0.45),
    "Weekday_Profile_Agent": (0.9, 0.45),
    "Insight_Manager_Agent": (0.5, 0.3),
    "Recommendation_Manager_Agent": (0.5, 0.15),
    "Recommendation_Critique_Agent": (0.5, 0.05)
}

# Adjust box width/height to fit names but avoid overlap
box_width = 0.24  # slightly narrower to prevent overlap
box_height = 0.1
for name, (x, y) in agents.items():
    rect = Rectangle((x - box_width/2, y - box_height/2), box_width, box_height,
                     fill=True, edgecolor='black', facecolor='#f0f0f0', linewidth=2)
    ax.add_patch(rect)
    ax.text(x, y, name, ha='center', va='center', fontsize=12)

# Function to draw straight arrows between agents
def draw_arrow(src, dst):
    sx, sy = agents[src]
    dx, dy = agents[dst]
    ax.annotate('', xy=(dx, dy + box_height/2 - 0.01),
                xytext=(sx, sy - box_height/2 + 0.01),
                arrowprops=dict(arrowstyle='->', lw=2))

# Build connections using draw_arrow
for agent in [
    "Plan_Ingestion_Agent", "Metadata_Ingestion_Agent",
    "Daily_Stats_Ingestion_Agent", "All_Merchants_Segmentation_Agent"
]:
    draw_arrow(agent, "Data_Ingestion_Manager_Agent")

draw_arrow("Data_Ingestion_Manager_Agent", "Orchestrator_Agent")

for agent in [
    "Sales_Trend_Agent", "Transaction_Trend_Agent",
    "ATS_Trend_Agent", "Peer_Stats_Agent", "Weekday_Profile_Agent"
]:
    draw_arrow("Orchestrator_Agent", agent)
    draw_arrow(agent, "Insight_Manager_Agent")

draw_arrow("Insight_Manager_Agent", "Recommendation_Manager_Agent")
draw_arrow("Recommendation_Manager_Agent", "Recommendation_Critique_Agent")

# Save the diagram
plt.savefig('/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/structured_framework.png', dpi=150, bbox_inches='tight')

