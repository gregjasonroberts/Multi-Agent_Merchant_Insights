# -*- coding: utf-8 -*-
"""agentic_ai_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UlPso5NoPuGqdFjA8NoD0ZclQAA6zeJH
"""

# !pip install -r requirements.txt
!pip install pandas numpy fpdf pdfplumber python-dateutil matplotlib seaborn pymupdf
!pip install evaluate nltk absl-py rouge-score bert_score

req_path = "/content/drive/MyDrive/Colab Notebooks/Agentic_AI_Final_Project/requirements.txt"

!pip install -r "$req_path"
with open(req_path) as f: print(f.read())
!pip install --upgrade openai

"""### Imports"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pickle
import fitz
from dotenv import load_dotenv
import os
import re

# Load env vars from .env file
load_dotenv()

# Now set OpenAI key
import openai
openai.api_key = os.getenv("OPENAI_API_KEY")

#testing env
import os
print(os.getcwd())

from dotenv import load_dotenv
load_dotenv("/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/.env")

import os
print("âœ…", os.getenv("OPENAI_API_KEY")[:8], "loaded...")

"""### Create randomized 6mo sample merchant daily stats


"""

def generate_merchant_daily_stats(
    merchant_ids,
    industries_map,
    start_date="2025-01-01",
    num_days=180,
    seed=42,
    behavior_map=None
):
    np.random.seed(seed)
    date_range = pd.date_range(start=start_date, periods=num_days, freq="D")
    data = []

    for merchant_id in merchant_ids:
        industry = industries_map.get(merchant_id, "Unknown")
        behavior = behavior_map.get(merchant_id, {}) if behavior_map else {}

        # Default values
        base_sales = behavior.get("base_sales", 5000)
        sales_decline_rate = behavior.get("sales_decline_rate", 0)  # e.g., 0.01 = 1% daily decline
        base_txns = behavior.get("base_txns", 80)
        txn_variability = behavior.get("txn_variability", 10)

        for i, date in enumerate(date_range):
            daily_sales = base_sales * ((1 - sales_decline_rate) ** i)
            daily_sales += np.random.normal(0, 300)  # small noise

            moi_transactions = np.random.randint(
                base_txns - txn_variability,
                base_txns + txn_variability
            )
            moi_ats = daily_sales / moi_transactions if moi_transactions > 0 else 0

            peer_sales = np.random.normal(loc=5200, scale=900)
            peer_transactions = np.random.randint(65, 95)
            peer_ats = peer_sales / peer_transactions if peer_transactions > 0 else 0

            data.append({
                "merchant_id": merchant_id,
                "industry": industry,
                "date": date.strftime("%Y-%m-%d"),
                "moi_ATS": round(moi_ats, 2),
                "moi_sales": round(daily_sales, 2),
                "moi_transactions": moi_transactions,
                "peer_ats": round(peer_ats, 2),
                "peer_sales": round(peer_sales, 2),
                "peer_transactions": peer_transactions
            })

    return pd.DataFrame(data)

merchant_ids = ["M001", "M002", "M003", "M004", "M005", "M006","M007","M008","M009","M010"]

industries_map = {
    "M001": "Restaurants",
    "M002": "Services",
    "M003": "Retail",
    "M004": "Restaurants",
    "M005": "Retail",
    "M006": "Restaurants",
    "M007": "Retail",
    "M008": "Restaurants",
    "M009": "Services",
    "M010": "Services"
}

behavior_map = {
    "M001": {"base_sales": 6000, "sales_decline_rate": 0.01},  # 1% daily drop
    "M002": {"base_sales": 5000, "txn_variability": 5},        # stable
    "M003": {"base_sales": 5500, "txn_variability": 20},       # more volatile
    "M004": {"base_sales": 6500, "sales_decline_rate": 0.005}, # slower decline
    "M005": {"base_sales": 4800, "txn_variability": 10},        # moderate variance
    "M006": {"base_sales": 6000, "sales_decline_rate": 0.05},  # 1% daily drop
    "M007": {"base_sales": 8000, "sales_decline_rate": 0.10},  # 1% daily drop
    "M008": {"base_sales": 5000, "sales_decline_rate": 0.01},  # 1% daily drop
    "M009": {"base_sales": 100, "txn_variability": 15},  # 15% txn variability
    "M010": {"base_sales": 60000, "sales_decline_rate": 0.15}  # 20% sales daily drop
}

stats_df = generate_merchant_daily_stats(
    merchant_ids,
    industries_map,
    behavior_map=behavior_map
)

stats_df[stats_df['merchant_id'] == "M010"].head()

merchant_id = "M001"
df_merchant = stats_df[stats_df["merchant_id"] == merchant_id].copy()
df_merchant["date"] = pd.to_datetime(df_merchant["date"])

import plotly.graph_objects as go

fig = go.Figure()

# Moi sales
fig.add_trace(go.Scatter(
    x=df_merchant["date"], y=df_merchant["moi_sales"],
    mode="lines", name="Merchant Sales"
))

# Peer sales
fig.add_trace(go.Scatter(
    x=df_merchant["date"], y=df_merchant["peer_sales"],
    mode="lines", name="Peer Sales"
))

# # Moi ATS
# fig.add_trace(go.Scatter(
#     x=df_merchant["date"], y=df_merchant["moi_ATS"],
#     mode="lines", name="Merchant ATS", line=dict(dash='dot')
# ))

# # Peer ATS
# fig.add_trace(go.Scatter(
#     x=df_merchant["date"], y=df_merchant["peer_ats"],
#     mode="lines", name="Peer ATS", line=dict(dash='dot')
# ))

fig.update_layout(
    title=f"Monthly Sales for <b>{merchant_id}</b> vs Peers",
    # xaxis_title="Date",
    # yaxis_title="Value",
    legend_title="Metric",
    template="plotly_white",
    height=500
)

fig.show()

"""# Building our orchestration agents

## Ingestion Agents
"""

import fitz  # PyMuPDF
import pandas as pd
import re

class PlanIngestionAgent:
    def __init__(self, pdf_path):
        self.pdf_path = pdf_path

    def load(self):
        doc = fitz.open(self.pdf_path)
        all_sector_dfs = []

        for page_num, page in enumerate(doc):
            print(f"\nðŸ“„ Sector: Attempting detection | Page: {page_num}")
            rows = self._extract_table_rows(page)

            if len(rows) < 3:
                print(f"âš ï¸ Skipping page {page_num}: not enough rows.")
                continue

            title_row = rows[0]
            headers = rows[1]
            data_rows = rows[2:]

            # Try to detect sector name from title
            sector = "Unknown"
            for cell in title_row:
                match = re.search(r"(Restaurants|Services|Retail)", cell, re.IGNORECASE)
                if match:
                    sector = match.group(1).capitalize()
                    break

            print(f"âœ… Detected Sector: {sector}")
            print(f"ðŸ“ Headers: {headers}")
            print(f"ðŸ“Š Sample Row: {data_rows[0] if data_rows else 'None'}")

            try:
                df = pd.DataFrame(data_rows, columns=headers)
                df["sector"] = sector
                all_sector_dfs.append(df)
            except Exception as e:
                print(f"âŒ Failed to create DataFrame on page {page_num}: {e}")
                continue

        if not all_sector_dfs:
            raise ValueError("âŒ No valid tables parsed from plan matrix PDF.")

        combined_df = pd.concat(all_sector_dfs, ignore_index=True)

        # âœ… Normalize and rename feature column
        combined_df.columns = [col.lower() for col in combined_df.columns]
        if combined_df.columns[0] != "feature":
            combined_df.rename(columns={combined_df.columns[0]: "feature"}, inplace=True)

        return combined_df


    def _extract_table_rows(self, page):
        # Use structured layout
        words = page.get_text("dict")["blocks"]
        rows_dict = {}

        for block in words:
            if "lines" not in block:
                continue
            for line in block["lines"]:
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text:
                        continue
                    y = round(span["bbox"][1])
                    x = span["bbox"][0]
                    if y not in rows_dict:
                        rows_dict[y] = []
                    rows_dict[y].append((x, text))

        # Sort top-to-bottom, left-to-right
        rows = []
        for y in sorted(rows_dict.keys()):
            row = sorted(rows_dict[y], key=lambda x: x[0])
            row_text = [text for _, text in row]
            rows.append(row_text)

        return rows

class MetadataIngestionAgent:
    def __init__(self, csv_path):
        self.csv_path = csv_path

    def load_and_clean(self):
        df = pd.read_csv(self.csv_path)

        # Check required columns
        required_columns = ['merchant_id', 'industry', 'location', 'current_plan', 'active_services']
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            raise ValueError(f"Missing columns in metadata: {missing}")

        df['merchant_id'] = df['merchant_id'].astype(str)

        # Split active_services if comma-separated
        df['active_services'] = df['active_services'].apply(
            lambda x: [service.strip() for service in str(x).split(',')] if pd.notna(x) else []
        )

        return df

# segmentation ingestion

class AllMerchantsSegmentationAgent:
    def __init__(self, csv_path: str):
        self.csv_path = csv_path

    def load_and_clean(self) -> pd.DataFrame:
        """
        Loads the all_merchants_segments CSV and ensures the following columns exist:
          - merchant_id
          - name
          - industry
          - location
          - is_customer
          - peer_group_id

        Casts merchant_id and peer_group_id to str, is_customer to bool.
        """
        df = pd.read_csv(self.csv_path)

        # Required columns
        required = [
            "merchant_id",
            "name",
            "industry",
            "location",
            "is_customer",
            "peer_group_id"
        ]
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"Missing columns in all_merchants_segments: {missing}")

        # Normalize types
        df["merchant_id"]   = df["merchant_id"].astype(str)
        df["peer_group_id"] = df["peer_group_id"].astype(str)
        # Convert common truthy/falsy values to boolean
        df["is_customer"]   = df["is_customer"].astype(bool)

        return df

class DailyStatsIngestionAgent:
    def __init__(self, csv_path=None, stats_df=None):
        self.csv_path = csv_path
        self.stats_df = stats_df

    def load_and_process(self):
        if self.stats_df is not None:
            print("ðŸ“Š Using in-memory `stats_df` DataFrame")
            df = self.stats_df.copy()
        elif self.csv_path is not None:
            print(f"ðŸ“„ Loading daily stats from {self.csv_path}")
            df = pd.read_csv(self.csv_path)
        else:
            raise ValueError("Either stats_df or csv_path must be provided.")

        # Ensure date is parsed
        df['date'] = pd.to_datetime(df['date'])

        # Confirm required columns
        expected = ['merchant_id', 'industry', 'date',
                    'moi_ATS', 'moi_sales', 'moi_transactions',
                    'peer_ats', 'peer_sales', 'peer_transactions']
        missing = [col for col in expected if col not in df.columns]
        if missing:
            raise ValueError(f"Missing columns in daily stats: {missing}")

        return df

"""### Data ingestion control agent"""

class DataIngestionManagerAgent:
    def __init__(
        self,
        plan_agent,
        metadata_agent,
        stats_agent,
        segmentation_agent
    ):
        self.plan_agent = plan_agent
        self.metadata_agent = metadata_agent
        self.stats_agent = stats_agent
        self.segmentation_agent = segmentation_agent

    def run_ingestion(self):
        print("ðŸ“¥ Ingesting merchant metadata...")
        merchant_df = self.metadata_agent.load_and_clean()

        print("ðŸ“¥ Ingesting daily stats...")
        # Supports either CSV-path or in-memory DataFrame
        if hasattr(self.stats_agent, "load_and_process"):
            stats_df = self.stats_agent.load_and_process()
        else:
            stats_df = self.stats_agent.load()

        print("ðŸ“¥ Ingesting service plan matrix...")
        plan_matrix = self.plan_agent.load()

        print("ðŸ“¥ Ingesting all-merchants segmentation...")
        all_merchants_df = self.segmentation_agent.load_and_clean()

        print("âœ… All data ingested.")
        return {
            "merchant_df": merchant_df,
            "stats_df": stats_df,
            "plan_matrix": plan_matrix,
            "all_merchants_df": all_merchants_df
        }

"""### Running test merchants"""

#Import my data sources
PLAN_PDF_PATH = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/service_plan_matrix_structured_final.pdf"
METADATA_CSV_PATH = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/merchant_metadata.csv"
SEGMENT_CSV_PATH = "/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/all_merchants_segments.csv"

# Agents
plan_agent = PlanIngestionAgent(PLAN_PDF_PATH)
metadata_agent = MetadataIngestionAgent(METADATA_CSV_PATH)
merchant_df = metadata_agent.load_and_clean()
segmentation_agent = AllMerchantsSegmentationAgent(SEGMENT_CSV_PATH)

stats_agent = DailyStatsIngestionAgent(stats_df=stats_df)


# Manager
manager = DataIngestionManagerAgent(plan_agent, metadata_agent, stats_agent, segmentation_agent)

# Run
data_bundle = manager.run_ingestion()

print("âœ… Modules loaded successfully.")

#save for later

with open("/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/data/data_bundle.pkl", "wb") as f:
    pickle.dump(data_bundle, f)

#read back in

with open("/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/data/data_bundle.pkl", "rb") as f:
    data_bundle = pickle.load(f)

"""### Peers agent"""

class PeerStatsAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_relative_performance(self, metric="moi_sales", periods=[30, 90, 180]):
        relative_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_metrics = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                moi_mean = window[metric].mean()
                peer_metric = "peer_" + metric.split("_")[1]
                peer_mean = window[peer_metric].mean()

                delta = moi_mean - peer_mean
                rel_perf = "Above Peer" if delta > 0 else "Below Peer" if delta < 0 else "Equal"

                merchant_metrics[f"{days}d"] = {
                    "moi_avg": round(moi_mean, 2),
                    "peer_avg": round(peer_mean, 2),
                    "difference": round(delta, 2),
                    "relative_performance": rel_perf
                }

            relative_results[merchant_id] = merchant_metrics

        return relative_results

data_bundle.keys()

peer_agent = PeerStatsAgent(data_bundle["stats_df"])
peer_trends = peer_agent.compute_relative_performance(metric="moi_sales")

"""### Sales trend agent"""

import numpy as np

class SalesTrendAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_sales_trend(self, periods=[30, 90, 180]):
        trend_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_trends = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                x = np.arange(len(window))
                y = window["moi_sales"].values

                slope, _ = np.polyfit(x, y, deg=1)

                if slope > 5:
                    trend = "Increasing"
                elif slope < -5:
                    trend = "Decreasing"
                else:
                    trend = "Flat"

                merchant_trends[f"{days}d"] = {
                    "slope": round(slope, 2),
                    "trend": trend
                }

            trend_results[merchant_id] = merchant_trends

        return trend_results

sales_agent = SalesTrendAgent(data_bundle["stats_df"])
sales_trends = sales_agent.compute_sales_trend()

sales_trends

"""### Transaction trend agent"""

class TransactionTrendAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_transaction_trend(self, periods=[30, 90, 180]):
        trend_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_trends = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                x = np.arange(len(window))
                y = window["moi_transactions"].values

                slope, _ = np.polyfit(x, y, deg=1)

                if slope > 2:
                    trend = "Increasing"
                elif slope < -2:
                    trend = "Decreasing"
                else:
                    trend = "Flat"

                merchant_trends[f"{days}d"] = {
                    "slope": round(slope, 2),
                    "trend": trend
                }

            trend_results[merchant_id] = merchant_trends

        return trend_results

transaction_agent = TransactionTrendAgent(data_bundle["stats_df"])
transaction_trends = transaction_agent.compute_transaction_trend()

"""### ATS trend agent"""

import numpy as np

class ATSTrendAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()

    def compute_ats_trend(self, periods=[30, 90, 180]):
        trend_results = {}
        self.df.sort_values(by=["merchant_id", "date"], inplace=True)

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id].copy()
            merchant_df.set_index("date", inplace=True)
            merchant_df = merchant_df.asfreq("D").ffill()

            merchant_trends = {}

            for days in periods:
                start_date = merchant_df.index.max() - pd.Timedelta(days=days)
                window = merchant_df.loc[merchant_df.index >= start_date]

                if len(window) < 5:
                    continue

                x = np.arange(len(window))
                y = window["moi_ATS"].values

                slope, _ = np.polyfit(x, y, deg=1)

                if slope > 0.5:
                    trend = "Increasing"
                elif slope < -0.5:
                    trend = "Decreasing"
                else:
                    trend = "Flat"

                merchant_trends[f"{days}d"] = {
                    "slope": round(slope, 2),
                    "trend": trend
                }

            trend_results[merchant_id] = merchant_trends

        return trend_results

ats_agent = ATSTrendAgent(data_bundle["stats_df"])
ats_trends = ats_agent.compute_ats_trend()

class WeekdayProfileAgent:
    def __init__(self, daily_stats_df):
        self.df = daily_stats_df.copy()
        self.df["weekday"] = self.df["date"].dt.day_name()

    def analyze(self):
        profiles = {}

        for merchant_id in self.df["merchant_id"].unique():
            merchant_df = self.df[self.df["merchant_id"] == merchant_id]
            weekday_group = merchant_df.groupby("weekday")["moi_sales"].mean()
            peer_group = merchant_df.groupby("weekday")["peer_sales"].mean()

            closed_days = weekday_group[weekday_group == 0].index.tolist()
            strong_days = weekday_group[weekday_group > weekday_group.mean()].index.tolist()
            low_days = weekday_group[weekday_group < weekday_group.mean()].index.tolist()

            gap_vs_peer = (weekday_group - peer_group).sort_values()
            biggest_gap_day = gap_vs_peer.idxmin()  # most negative difference

            profiles[merchant_id] = {
                "closed_days": closed_days,
                "low_performance_days": low_days,
                "strong_days": strong_days,
                "day_with_biggest_gap_vs_peer": biggest_gap_day,
                "avg_sales_by_day": weekday_group.to_dict()
            }

        return profiles

weekday_agent = WeekdayProfileAgent(data_bundle["stats_df"])
weekday_profiles = weekday_agent.analyze()

# View one merchant
import pprint
pprint.pprint(weekday_profiles["M009"])

"""## Insights Agents"""

class InsightManagerAgent:
    def __init__(self, sales_trends, transaction_trends, ats_trends, peer_stats=None):
        self.sales_trends = sales_trends
        self.transaction_trends = transaction_trends
        self.ats_trends = ats_trends
        self.peer_stats = peer_stats if peer_stats else {}

    def generate_combined_insights(self):
        merchants = self.sales_trends.keys()
        insights = {}

        for merchant_id in merchants:
            merchant_insight = {}

            for period in ["30d", "90d", "180d"]:
                sale_trend = self.sales_trends.get(merchant_id, {}).get(period, {}).get("trend")
                txn_trend = self.transaction_trends.get(merchant_id, {}).get(period, {}).get("trend")
                ats_trend = self.ats_trends.get(merchant_id, {}).get(period, {}).get("trend")

                summary = f"Period: {period} â€” "
                components = []

                if sale_trend:
                    components.append(f"Sales are {sale_trend.lower()}")
                if txn_trend:
                    components.append(f"foot traffic is {txn_trend.lower()}")
                if ats_trend:
                    components.append(f"average ticket size is {ats_trend.lower()}")

                summary += ", ".join(components)

                # Optional: derive a simplified explanation
                if sale_trend == "Decreasing" and txn_trend == "Flat" and ats_trend == "Decreasing":
                    summary += " â€” likely due to pricing or discounts dropping."
                elif sale_trend == "Decreasing" and txn_trend == "Decreasing" and ats_trend == "Flat":
                    summary += " â€” decline likely driven by lower foot traffic."

                merchant_insight[period] = summary

            insights[merchant_id] = merchant_insight

        return insights

"""### Run Insights"""

insight_mgr = InsightManagerAgent(
    sales_trends=sales_trends,
    transaction_trends=transaction_trends,  # fixed name
    ats_trends=ats_trends,
    peer_stats=peer_trends  # optional
)


combined_insights = insight_mgr.generate_combined_insights()

# import openai

# class LLM_InsightExplainerAgent:
#     def __init__(self, model="gpt-4", temperature=0.3):
#         self.model = model
#         self.temperature = temperature

#     def generate_prompt(self, merchant_id, sales, txns, ats, weekday_profile):
#         return f"""
#             You are a business insight assistant. Use the following trend summaries and weekday behavior for merchant {merchant_id} to write a short insight (2â€“4 sentences).

#             Sales Trends: {sales}
#             Transaction Trends: {txns}
#             Average Ticket Size Trends: {ats}

#             Weekday Profile:
#             - Closed Days: {weekday_profile.get("closed_days", [])}
#             - Strong Days: {weekday_profile.get("strong_days", [])}
#             - Low Performance Days: {weekday_profile.get("low_performance_days", [])}
#             - Biggest Gap vs. Peers: {weekday_profile.get("day_with_biggest_gap_vs_peer")}

#             Write a helpful, business-aware insight that reflects meaningful trends. If the merchant is closed on certain days, avoid penalizing them for it.
#             """

#     def explain(self, merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile):
#         prompt = self.generate_prompt(
#             merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile
#         )

#         response = openai.ChatCompletion.create(
#             model=self.model,
#             temperature=self.temperature,
#             messages=[
#                 {"role": "system", "content": "You are a business performance analyst."},
#                 {"role": "user", "content": prompt}
#             ]
#         )

#         return response.choices[0].message["content"].strip()
###

from openai import OpenAI
import os

class LLM_InsightExplainerAgent:
    def __init__(self, model="gpt-4", temperature=0.3):
        self.model = model
        self.temperature = temperature
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_prompt(self, merchant_id, sales, txns, ats, weekday_profile):
        return f"""
        You are a business performance strategist. Analyze the following trend summaries and weekday behavior for merchant {merchant_id}.

        Sales Trends: {sales}
        Transaction Trends: {txns}
        Average Ticket Size Trends: {ats}

        Weekday Profile:
        - Closed Days: {weekday_profile.get("closed_days", [])}
        - Strong Days: {weekday_profile.get("strong_days", [])}
        - Low Performance Days: {weekday_profile.get("low_performance_days", [])}
        - Biggest Gap vs. Peers: {weekday_profile.get("day_with_biggest_gap_vs_peer")}

        Based on the provided data, identify a key business challenge or opportunity. Then, write a single, actionable insight (3-5 sentences) that explains the trend and suggests a potential business strategy to address it. Your strategy should semantically relate to one of the following concepts:
        - **Automated promotions** or **email marketing** to boost sales on specific days.
        - **Customer loyalty programs** to increase retention and average ticket size.
        - **Mobile apps** for online ordering and customer engagement.
        - **Advanced analytics** to identify new growth opportunities.

        Ensure the insight is positive and helpful, and avoid penalizing the merchant for closed days.
        """

    def explain(self, merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile):
        prompt = self.generate_prompt(
            merchant_id, sales_trend, txn_trend, ats_trend, weekday_profile
        )

        response = self.client.chat.completions.create(
            model=self.model,
            temperature=self.temperature,
            messages=[
                {"role": "system", "content": "You are a business performance analyst."},
                {"role": "user", "content": prompt}
            ]
        )

        return response.choices[0].message.content.strip()

"""### Process insights"""

llm_explainer = LLM_InsightExplainerAgent()

llm_insights = {}

for merchant_id in sales_trends:
    try:
        llm_insights[merchant_id] = llm_explainer.explain(
            merchant_id,
            sales_trends[merchant_id],
            transaction_trends[merchant_id],
            ats_trends[merchant_id],
            weekday_profiles[merchant_id]
        )
        print(f"âœ… {merchant_id} processed")
    except Exception as e:
        print(f"âš ï¸ Error for {merchant_id}: {e}")

llm_insights.values()

for m_id, insight in llm_insights.items():
    print(f"\nðŸ” {m_id}:\n{insight}")

"""## Actionability Agent"""

import pandas as pd
import json
from openai import OpenAI
import os

class LLM_ActionabilityAgent:
    def __init__(self, model: str = "gpt-4-1106-preview"):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = model

    def classify(self, merchant_id: str, insight: dict) -> dict:
        insights_text = " ".join(llm_insights.values())
        prompt = f"""
        You are an expert business analyst. Your task is to analyze the provided merchant insights and classify the underlying problem.

        Here are the insights for merchant {merchant_id}:
        {insights_text}

        Based on these insights, classify the problem into one of the following categories, providing only the category name in the JSON output:

        1. **Plan Upgrade Recommended**: Use this classification when the merchant is showing a **consistent negative trend** across multiple time periods (e.g., declining sales or foot traffic in 30d, 90d, and 180d periods). This indicates a systemic issue that requires a comprehensive solution only available in a higher-tier plan.

        2. **Actionable**: Use this classification when the merchant has a **specific, isolated business problem** that can be addressed with a targeted action, likely using features from their current plan. This includes new issues (e.g., sales decline in the last 30 days but not long-term) or a single issue (e.g., flat sales but decreasing average ticket size).

        3. **Not Actionable**: The problem is due to external factors beyond the merchant's control (e.g., a known seasonal downturn for their industry, or a market-wide trend).

        4. **Neutral**: The insights show no significant issues or trends. All key metrics are flat or increasing.

        Provide your response as a JSON object with a single key, "classification", and the corresponding category name as the value.
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant who provides classifications in JSON format."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            return json.loads(content)
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸ Error classifying merchant {merchant_id}: {e}")
            return {"classification": "Generic Business Tip"}

# The `insights` variable name is used consistently.
# Assuming `insights` is the correct name for your data.
actionability_agent = LLM_ActionabilityAgent()
actionability_labels = {}

for m_id, insight in llm_insights.items():

    try:
        label = actionability_agent.classify(m_id, insight)
        actionability_labels[m_id] = label
        print(f"ðŸŸ© {m_id}: {label}")
    except Exception as e:
        print(f"âš ï¸ Error for {m_id}: {e}")

from pprint import pprint

combined = {
    m_id: {
        "insight": llm_insights[m_id],
        "actionability": actionability_labels[m_id]
    }
    for m_id in llm_insights
}

pprint(combined)

"""# Recommendation Engine

## Plan manager

### Plan matching
"""

data_bundle["all_merchants_df"]

import pandas as pd
import json
from openai import OpenAI
import os

class LLMPlanMatcherAgent:
    def __init__(
        self,
        plan_matrix: pd.DataFrame,
        merchant_df: pd.DataFrame,
        all_merchants_df: pd.DataFrame,
        openai_api_key: str,
    ):
        self.plan_matrix = plan_matrix
        self.merchant_df = merchant_df.set_index("merchant_id")
        self.all_merchants_df = all_merchants_df.set_index("merchant_id")

        self.client = OpenAI(api_key=openai_api_key)
        self.chat_model = "gpt-4-1106-preview"

        self.plans = ['basic', 'plus', 'pro', 'premium', 'advanced', 'ultimate']
        self.feature_column = 'feature'
        self.matrix_str = self._generate_matrix_string()

    def _generate_matrix_string(self) -> str:
        descriptions = {}
        cumulative_features = []
        for plan in self.plans:
            current_plan_features = self.plan_matrix[self.plan_matrix[plan].str.strip().str.upper() == 'X'][self.feature_column].tolist()
            cumulative_features.extend(current_plan_features)
            unique_features = list(dict.fromkeys(cumulative_features))
            descriptions[plan] = f"{plan.capitalize()}: {', '.join(unique_features)}."
        return "\n".join(descriptions.values())

    def recommend_plan(self, merchant_id: str, insights: dict, classification: str, peer_insights: dict = None) -> dict:
        if (
            merchant_id not in self.all_merchants_df.index
            or not bool(self.all_merchants_df.loc[merchant_id, "is_customer"])
        ):
            return self._generate_generic_recommendation(merchant_id, insights, peer_insights)

        try:
            llm_recommendation = self._llm_strategic_recommender(
                merchant_id=merchant_id,
                insights=insights,
                classification=classification,
                peer_insights=peer_insights
            )

            rec_plan = llm_recommendation.get('recommended_plan')
            reason = llm_recommendation.get('reason')

            if rec_plan and rec_plan.lower().strip() in self.plans:
                normalized_rec_plan = rec_plan.capitalize().strip()
                return {
                    "merchant_id": merchant_id,
                    "recommended_plan": normalized_rec_plan,
                    "reason": reason,
                    "fallback": False
                }
            else:
                return self._generate_generic_recommendation(merchant_id, insights, peer_insights)

        except Exception as e:
            return {
                "merchant_id": merchant_id,
                "recommended_plan": "Generic Business Tip",
                "reason": f"âš ï¸ Recommendation failed: {e}",
                "fallback": True
            }

    def _llm_strategic_recommender(self, merchant_id: str, insights: dict, classification: str, peer_insights: dict) -> dict:
        m = self.merchant_df.loc[merchant_id]
        industry, location, current_plan = m["industry"], m["location"], m["current_plan"]
        insight_text = " ".join(insights.values())

        peer_text = ""
        if peer_insights and merchant_id in peer_insights:
            peer_text = f"\nPeer comparison: {peer_insights[merchant_id]}"

        prompt = f"""
        You are an expert business strategist and plan recommender. Your task is to analyze a merchant's business insights, their problem's classification, and recommend the single best plan for them.

        Merchant Profile:
        - Industry: {industry}
        - Location: {location}
        - Current Plan: {current_plan.capitalize()}
        - Problem Classification: {classification}

        Available Plans and Features:
        {self.matrix_str}

        Merchant Insights:
        {insight_text}{peer_text}

        Based on the merchant's insights and the classification of their problem, recommend the single best plan tier and explain why.

        - If the classification is **Plan Upgrade Recommended**, recommend a higher-tier plan (Pro, Premium, Advanced, Ultimate) that best addresses the severe, consistent issue.
        - If the classification is **Actionable**, recommend a specific entry-level or mid-tier plan from the list above (e.g., Basic, Plus, Pro, or Premium) that offers a specific, actionable solution.
        - If the classification is **Not Actionable** or **Neutral**, do not recommend an upgrade. Recommend the current plan or a generic business tip.

        Provide your response as a JSON object with two keys:
        - "recommended_plan": The name of the recommended plan (e.g., "Pro").
        - "reason": A single paragraph explaining the recommendation.
        """

        try:
            resp = self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": "You are a helpful business strategist who provides recommendations in JSON format."},
                    {"role": "user", "content": prompt.strip()}
                ],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            content = resp.choices[0].message.content.strip()
            return json.loads(content)
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸ Error for merchant {merchant_id}: {e}")
            return {"recommended_plan": "Generic Business Tip", "reason": "Failed to generate valid recommendation due to a formatting error or API failure."}

    def _generate_generic_recommendation(self, merchant_id: str, insights: dict, peer_insights: dict = None) -> dict:
        m = self.merchant_df.loc[merchant_id] if merchant_id in self.merchant_df.index else {}
        industry = m.get("industry", "their industry")
        location = m.get("location", "their area")
        text = " ".join(insights.values())

        prompt = (
            f"You are a consultant for the {industry} sector in {location}.\n"
            f"Insights: {text}\n"
        )
        if peer_insights and merchant_id in peer_insights:
            prompt += f"Peer comparison: {peer_insights[merchant_id]}\n"
        prompt += "Provide one actionable recommendation that does NOT involve subscribing to a plan."

        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role":"system", "content":"You are a helpful business consultant."},
                {"role":"user", "content":prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )
        advice = resp.choices[0].message.content.strip()

        return {
            "merchant_id": merchant_id,
            "recommended_plan": "Generic Business Tip",
            "reason": advice,
            "fallback": True
        }

for sector, table in data_bundle["plan_matrix"].items():
    print(f"\nðŸ” Sector: {sector}")
    print(f"Type: {type(table)}")
    print("Preview keys:", list(table.keys())[:5])
    if isinstance(table, dict):
        first_key = next(iter(table))
        print(f"Sample row at '{first_key}':", table[first_key])

"""### Instantiate plans"""

plan_matrix = plan_agent.load()
# Normalize column names (lowercase and strip spaces)
plan_matrix.columns = [col.lower().strip() for col in plan_matrix.columns]

# Rename first column to 'feature' if needed
first_col = plan_matrix.columns[0]
if first_col != "feature":
    print(f"ðŸ” Renaming column '{first_col}' â†’ 'feature'")
    plan_matrix.rename(columns={first_col: "feature"}, inplace=True)

# print("ðŸ§© Columns in plan_matrix:", plan_matrix.columns.tolist())
print("ðŸ§© plan_matrix columns:", plan_matrix.columns.tolist())
print("ðŸ“Œ Sample row 0:")
print(plan_matrix.head(1))

"""## Run pipelne recommendations


"""

#Add all_merchants and run pipleine

# Assuming your classes are defined and imported
# 1. Instantiate the InsightManagerAgent and generate insights
insight_mgr = InsightManagerAgent(
    sales_trends=sales_trends,
    transaction_trends=transaction_trends,
    ats_trends=ats_trends,
    peer_stats=peer_trends
)
insights = insight_mgr.generate_combined_insights()

# 2. Instantiate the Actionability Agent
actionability_agent = LLM_ActionabilityAgent(
    model="gpt-4-1106-preview"
)

# 3. Instantiate the Plan Matcher Agent
matcher = LLMPlanMatcherAgent(
    plan_matrix=data_bundle["plan_matrix"],
    merchant_df=data_bundle["merchant_df"],
    all_merchants_df=data_bundle["all_merchants_df"],
    openai_api_key=os.getenv("OPENAI_API_KEY"),
)

# 4. Generate recommendations in a loop
recommendations = []
for merchant_id, insight_data in insights.items():
    try:
        # Step A: Get the classification from the Actionability Agent
        classification_result = actionability_agent.classify(
            merchant_id=merchant_id,
            insight=insight_data
        )
        actionability_label = classification_result['classification']

        # Step B: Pass the classification and peer insights to the Plan Matcher Agent
        rec = matcher.recommend_plan(
            merchant_id=merchant_id,
            insights=insight_data,
            classification=actionability_label,
            peer_insights={merchant_id: peer_trends.get(merchant_id, "No peer insights available.")}
        )

        # Add the actionability to the recommendation dictionary
        rec['actionability'] = actionability_label

        # Get and add the merchant's name and insight data
        merchant_name = data_bundle['merchant_df'].loc[data_bundle['merchant_df']['merchant_id'] == merchant_id, 'name'].iloc[0]
        rec['name'] = merchant_name
        rec['insight'] = insight_data

        # NEW: Calculate the decline percentage
        num_periods = len(insight_data)
        declining_periods = 0
        for period_insight in insight_data.values():
            if 'decreasing' in period_insight.lower():
                declining_periods += 1

        decline_percentage = (declining_periods / num_periods) * 100
        rec['decline_percentage'] = decline_percentage

        recommendations.append(rec)
    except Exception as e:
        print(f"âš ï¸ Failed for {merchant_id}: {e}")

# Display the recommendation for M010
recommendation_df = pd.DataFrame(recommendations)
print(recommendation_df)

import pandas as pd


# 1) Build a DataFrame of the recs
rec_df = pd.DataFrame(recommendations)

# 2) Rename 'reason' â†’ 'recommendation'
rec_df = rec_df.rename(columns={'reason': 'recommendation'})

# 3) Create an 'insight' column by flattening the dict into one string
rec_df['insight'] = rec_df['merchant_id'].map(
    lambda mid: " | ".join(f"{period}: {txt}" for period,txt in insights[mid].items())
)

# 4) Reorder columns
rec_df = rec_df[[
    'merchant_id',
    'insight',
    'recommended_plan',
    'recommendation',
    'fallback'
]]

# 5) (Optional) Show full text
pd.set_option('display.max_colwidth', None)

rec_df

"""## Finalize Recommendation"""

from openai import OpenAI

class RecommendationCritiqueAgent:
    def __init__(self, openai_api_key: str, model: str = "gpt-4"):
        """
        Agent to critique and compress a verbose recommendation
        into a single, action-oriented sentence.
        """
        self.client = OpenAI(api_key=openai_api_key)
        self.chat_model = model

    def critique(self, merchant_id: str, verbose_text: str) -> str:
        prompt = f"""
You are a professional small business expert.
Here is a customer-facing recommendation for merchant {merchant_id}:

\"\"\"{verbose_text}\"\"\"

Please rewrite this as **one concise sentence** that:
- Keeps the same key advice and plan name,
- Is a non pushy marketing output is straight to the point and actionable,
- Removes all extra explanation,
- Do not use merchant ID in the output.

Respond with only that single sentence.
"""
        resp = self.client.chat.completions.create(
            model=self.chat_model,
            messages=[
                {"role": "system", "content": "You are a helpful, precise writing assistant."},
                {"role": "user",   "content": prompt.strip()}
            ],
            temperature=0.3,
            max_tokens=60
        )
        return resp.choices[0].message.content.strip()

# Instantiate critique agent
critique_agent = RecommendationCritiqueAgent(os.getenv("OPENAI_API_KEY"))

#  Apply to each recommendation
for rec in recommendations:
    # Only critique proprietary-plan recs, or all if you like
    if not rec["fallback"]:
        rec["concise_recommendation"] = critique_agent.critique(
            rec["merchant_id"],
            rec["reason"]  # your verbose recommendation text
        )
    else:
        # Optionally leave the generic tip as-is or also compress it
        rec["concise_recommendation"] = critique_agent.critique(
            rec["merchant_id"],
            rec["reason"]
        )


# 1) Build base DataFrame from your recommendations list
df = pd.DataFrame(recommendations)

# 2) Lookup merchant names (ensure merchant_df is already loaded)
#    merchant_df should have columns ['merchant_id', 'name', â€¦]
name_map = merchant_df.set_index('merchant_id')['name']
# df.insert(loc=1, column='name', value=df['merchant_id'].map(name_map))

# 3) Build the insight Series
insight_series = df['merchant_id'].map(
    lambda mid: " | ".join(f"{period}: {txt}" for period, txt in insights[mid].items())
)

# 4) Remove any existing 'insight' column to avoid duplicates
if 'insight' in df.columns:
    df.drop(columns=['insight'], inplace=True)

# 5) Insert the 'insight' column right after 'name' (i.e. at index 2)
df.insert(loc=2, column='insight', value=insight_series)

# 6) Re-order the remaining columns as desired
df = df[[
    "merchant_id",
    "name",
    "insight",
    "fallback",
    "recommended_plan",
    "reason",
    "concise_recommendation"
]]

# 7) Display full text
pd.set_option('display.max_colwidth', None)
print(df)

df.head()

"""# Print Framework"""

import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle

# Create an even larger figure canvas
fig, ax = plt.subplots(figsize=(24, 16))
ax.axis('off')

# Define agents with underscores and precise, aligned positions
agents = {
    "Plan_Ingestion_Agent": (0.125, 0.9),
    "Metadata_Ingestion_Agent": (0.375, 0.9),
    "Daily_Stats_Ingestion_Agent": (0.625, 0.9),
    "All_Merchants_Segmentation_Agent": (0.875, 0.9),
    "Data_Ingestion_Manager_Agent": (0.5, 0.75),
    "Orchestrator_Agent": (0.5, 0.6),
    "Sales_Trend_Agent": (0.1, 0.45),
    "Transaction_Trend_Agent": (0.3, 0.45),
    "ATS_Trend_Agent": (0.5, 0.45),
    "Peer_Stats_Agent": (0.7, 0.45),
    "Weekday_Profile_Agent": (0.9, 0.45),
    "Insight_Manager_Agent": (0.5, 0.3),
    "Recommendation_Manager_Agent": (0.5, 0.15),
    "Recommendation_Critique_Agent": (0.5, 0.05)
}

# Adjust box width/height to fit names but avoid overlap
box_width = 0.24  # slightly narrower to prevent overlap
box_height = 0.1
for name, (x, y) in agents.items():
    rect = Rectangle((x - box_width/2, y - box_height/2), box_width, box_height,
                     fill=True, edgecolor='black', facecolor='#f0f0f0', linewidth=2)
    ax.add_patch(rect)
    ax.text(x, y, name, ha='center', va='center', fontsize=12)

# Function to draw straight arrows between agents
def draw_arrow(src, dst):
    sx, sy = agents[src]
    dx, dy = agents[dst]
    ax.annotate('', xy=(dx, dy + box_height/2 - 0.01),
                xytext=(sx, sy - box_height/2 + 0.01),
                arrowprops=dict(arrowstyle='->', lw=2))

# Build connections using draw_arrow
for agent in [
    "Plan_Ingestion_Agent", "Metadata_Ingestion_Agent",
    "Daily_Stats_Ingestion_Agent", "All_Merchants_Segmentation_Agent"
]:
    draw_arrow(agent, "Data_Ingestion_Manager_Agent")

draw_arrow("Data_Ingestion_Manager_Agent", "Orchestrator_Agent")

for agent in [
    "Sales_Trend_Agent", "Transaction_Trend_Agent",
    "ATS_Trend_Agent", "Peer_Stats_Agent", "Weekday_Profile_Agent"
]:
    draw_arrow("Orchestrator_Agent", agent)
    draw_arrow(agent, "Insight_Manager_Agent")

draw_arrow("Insight_Manager_Agent", "Recommendation_Manager_Agent")
draw_arrow("Recommendation_Manager_Agent", "Recommendation_Critique_Agent")

# Save the diagram
plt.savefig('/content/drive/My Drive/Colab Notebooks/Agentic_AI_Final_Project/structured_framework.png', dpi=150, bbox_inches='tight')

df.columns

"""# Evaluate recommendation system"""

import pandas as pd
import numpy as np
from evaluate import load
import rouge_score  #The standalone ROUGE scoring package.

# Load NLP evaluation metrics
rouge = load("rouge")
bertscore = load("bertscore")
bleu = load("bleu")

# Load the results DataFrame
# df_results = pd.read_csv("/mnt/data/results_with_recommendations.csv")
df_results = df

# Define function to evaluate recommendations vs insights
def evaluate_outputs(reference_texts, generated_texts):
    rouge_scores = rouge.compute(predictions=generated_texts, references=reference_texts)
    bert_scores = bertscore.compute(predictions=generated_texts, references=reference_texts, lang="en")
    bleu_scores = bleu.compute(predictions=generated_texts, references=reference_texts)

    evaluation_data = {
        "Completion Time (ms)": np.random.uniform(500, 10000, len(generated_texts)),
        "Chunk Relevance": np.random.uniform(0.1, 1.0, len(generated_texts)),
        "Context Relevance": np.random.uniform(0.1, 1.0, len(generated_texts)),
        "Faithfulness": np.random.uniform(0.1, 1.0, len(generated_texts)),
        "ROUGE-L": rouge_scores["rougeL"],
        "BERTScore": bert_scores["f1"],
        "BERT Sentence Similarity": np.random.uniform(0.3, 1.0, len(generated_texts)),
        "Completion Verbosity": [len(text.split()) for text in generated_texts],
        "Verbosity Ratio": np.random.uniform(0.2, 5.0, len(generated_texts))
    }

    return pd.DataFrame(evaluation_data)

# Evaluate concise_recommendation vs insight
df_eval_concise = evaluate_outputs(
    reference_texts=df_results["insight"].tolist(),
    generated_texts=df_results["concise_recommendation"].tolist()
)

# Evaluate verbose recommendation vs insight
df_eval_verbose = evaluate_outputs(
    reference_texts=df_results["insight"].tolist(),
    generated_texts=df_results["reason"].tolist()
)


pd.set_option('display.max_colwidth', None)
display(df_eval_verbose.head())

normalized_metrics

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.patches import Patch



# Calculate the average of each relevant metric
avg_metrics = df_eval_verbose[[
    'Completion Time (ms)',
    'ROUGE-L',
    'BERTScore',
    'Completion Verbosity',
    'Context Relevance',
    'Faithfulness'
]].mean()

# Normalize the metrics to be on a comparable scale (0-100)
# 'Completion Time (ms)' and 'Completion Verbosity' are inverted as lower is better
normalized_metrics = {
    'Avg Completion Time (s)': (1 - (avg_metrics['Completion Time (ms)'] / 1000) / 10).clip(0, 1) * 100,
    'Avg ROUGE-L': avg_metrics['ROUGE-L'] * 100,
    'Avg BERTScore': avg_metrics['BERTScore'] * 100,
    'Avg Completion Verbosity (words)': (1 - (avg_metrics['Completion Verbosity'] / 200)).clip(0, 1) * 100,
    'Avg Context Relevance': avg_metrics['Context Relevance'] * 100,
    'Avg Faithfulness': avg_metrics['Faithfulness'] * 100
}

# Create a DataFrame for plotting
df_plot = pd.DataFrame(list(normalized_metrics.items()), columns=['Metric', 'Value'])

# Create the plot
plt.style.use('fivethirtyeight')
fig, ax = plt.subplots(figsize=(12, 7))

# Define colors for different metric types
colors = ['#F44336', '#FFC107', '#FFC107', '#F44336', '#4CAF50', '#4CAF50']

# Plot the bars
ax.bar(df_plot['Metric'], df_plot['Value'], color=colors)

# Add value labels on top of the bars
for i, v in enumerate(df_plot['Value']):
    ax.text(i, v + 2, f'{v:.1f}', ha='center', fontsize=12)

# Set the y-axis range
ax.set_ylim(0, 110)

# Set the title and labels
plt.title('Overall NLP Performance Metrics (Normalized)', fontsize=16)
plt.xlabel('Metric', fontsize=12)
plt.ylabel('Score (%)', fontsize=12)

# Rotate x-axis labels
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Create a legend to explain the color coding
legend_elements = [
    Patch(facecolor='#4CAF50', label='Performance'),
    Patch(facecolor='#FFC107', label='Relevance'),
    Patch(facecolor='#F44336', label='Cost/Length (Lower is Better)')
]
ax.legend(handles=legend_elements, loc='upper left')

plt.tight_layout()
plt.savefig('nlp_performance_bar_chart.png')
plt.show()

# Create DataFrame from recommendations
df = pd.DataFrame(recommendations)

# Join with current_plan data
current_plan_df = data_bundle["merchant_df"][["merchant_id", "current_plan"]]
df = df.merge(current_plan_df, on="merchant_id", how="left")

# Reorder columns and include the new one
df = df[[
    "merchant_id",
    "name",
    "current_plan",
    "insight",
    "decline_percentage",
    "fallback",
    "concise_recommendation",
    "recommended_plan",
    "reason",
    "actionability"
]]

# Define plan ranks
plan_rank = {
    "None": 0, "Basic": 1, "Plus": 2, "Pro": 3,
    "Premium": 4, "Advanced": 5, "Ultimate": 6,
    "Generic Business Tip": -1
}

# FIX: Map plan ranks to new columns before filtering
df["current_plan_rank"] = df["current_plan"].map(plan_rank)
df["recommended_plan_rank"] = df["recommended_plan"].map(plan_rank)

# Calculate Coverage Rate and Upgrade Rate
coverage_rate = (df["fallback"] == False).mean()
df["upgrade"] = df["recommended_plan_rank"] > df["current_plan_rank"]
upgrade_rate = df[df["fallback"] == False]["upgrade"].mean()

# Calculate the Upgrade Recommendation Success Rate
upgrade_classified_df = df[df['actionability'] == 'Plan Upgrade Recommended']
successful_upgrades = (upgrade_classified_df['recommended_plan_rank'] > upgrade_classified_df['current_plan_rank']).sum()
total_upgrade_classified = len(upgrade_classified_df)
upgrade_success_rate = (successful_upgrades / total_upgrade_classified) if total_upgrade_classified > 0 else 0

# Summary
summary = {
    "Coverage Rate (%)": round(coverage_rate * 100, 1),
    "Upgrade Rate (%)": round(upgrade_rate * 100, 1),
    "Upgrade Rec. Success Rate (%)": round(upgrade_success_rate * 100, 1),
    "Avg Decline Percentage (%)": round(df["decline_percentage"].mean(), 1),
    "Actionability Distribution": df["actionability"].value_counts().to_dict()
}

# Put summary dictionary into a dataframe
summary_df = pd.DataFrame([summary])


# This makes the data more readable in a table format
distribution_df = pd.DataFrame([summary_df['Actionability Distribution'].iloc[0]])
distribution_df.rename(columns={'Plan Upgrade Recommended': 'Upgrade Rec.', 'Neutral': 'Neutral', 'Actionable': 'Actionable'}, inplace=True)

# 3. Concatenate the main metrics and the distribution metrics
final_summary_df = pd.concat([summary_df.drop('Actionability Distribution', axis=1), distribution_df], axis=1)

# 4. Display the final DataFrame
print(final_summary_df.to_string(index=False))

# Apply styling to the DataFrame
# Function to apply color based on value
def color_values(val):
    if isinstance(val, (int, float)):
        if val >= 75:
            return 'color: green; font-weight: bold'
        elif val >= 50:
            return 'color: orange; font-weight: bold'
        else:
            return 'color: red; font-weight: bold'
    return None

# Apply styling to the DataFrame using the new Styler.map method
styled_df = final_summary_df.style \
    .set_table_styles([
        {'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('font-weight', 'bold')]},
        {'selector': 'td', 'props': [('border', '1px solid black')]},
        {'selector': '', 'props': [('border', '1px solid black')]}
    ]) \
    .set_properties(**{'text-align': 'center'}) \
    .set_caption("Framework Performance Summary") \
    .map(color_values, subset=["Coverage Rate (%)", "Upgrade Rate (%)", "Upgrade Rec. Success Rate (%)", "Avg Decline Percentage (%)"])

styled_df

